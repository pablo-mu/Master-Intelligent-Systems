{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SPARK NLP (John Snow Labs)\n",
        "\n",
        " Two alternatives for installation:\n",
        "\n",
        "- Colab: `!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash`\n",
        "\n",
        "- Jupyter/local:\n",
        "```\n",
        "!pip install pyspark\n",
        "!pip install spark-nlp==5.1.4\n",
        "```\n",
        "\n",
        "More info and examples: https://github.com/JohnSnowLabs/spark-nlp-workshop\n",
        "\n"
      ],
      "metadata": {
        "id": "2WRaqe-V1ojk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrVw1ZP310el",
        "outputId": "7956153e-c1b7-4db0-c3ef-b2a81b164554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-01 12:25:54--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2023-11-01 12:25:55--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-01 12:25:55 (78.8 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 5.1.4\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.1.4\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.7/540.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buildig the context and spark object"
      ],
      "metadata": {
        "id": "INmXlS3Q0bxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "u7PNPloz2Bev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdALmNEZ2cwG",
        "outputId": "c165aee7-bfb4-4c4a-d232-981567ffeccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version: 5.1.4\n",
            "Apache Spark version: 3.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "metadata": {
        "id": "GpPTViyN2nUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.serializers import NoOpSerializer\n",
        "ner = PretrainedPipeline('recognize_entities_dl', 'en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RSXoyMg2nz-",
        "outputId": "11c99ec5-26fc-476b-847d-aafc967b752a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recognize_entities_dl download started this may take some time.\n",
            "Approx size to download 159 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = ner.annotate('The president Jesús Martínez arrived yesterday at Santa Cruz de Tenerife and he gave a nice speech.')"
      ],
      "metadata": {
        "id": "WDQnp0xd2rYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_aGo9rx2_Py",
        "outputId": "bf689bfe-cddf-4481-9087-1ff549dd5d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'entities': ['Jesús Martínez', 'Santa Cruz de Tenerife'],\n",
              " 'document': ['The president Jesús Martínez arrived yesterday at Santa Cruz de Tenerife and he gave a nice speech.'],\n",
              " 'token': ['The',\n",
              "  'president',\n",
              "  'Jesús',\n",
              "  'Martínez',\n",
              "  'arrived',\n",
              "  'yesterday',\n",
              "  'at',\n",
              "  'Santa',\n",
              "  'Cruz',\n",
              "  'de',\n",
              "  'Tenerife',\n",
              "  'and',\n",
              "  'he',\n",
              "  'gave',\n",
              "  'a',\n",
              "  'nice',\n",
              "  'speech',\n",
              "  '.'],\n",
              " 'ner': ['O',\n",
              "  'O',\n",
              "  'B-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'I-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " 'embeddings': ['The',\n",
              "  'president',\n",
              "  'Jesús',\n",
              "  'Martínez',\n",
              "  'arrived',\n",
              "  'yesterday',\n",
              "  'at',\n",
              "  'Santa',\n",
              "  'Cruz',\n",
              "  'de',\n",
              "  'Tenerife',\n",
              "  'and',\n",
              "  'he',\n",
              "  'gave',\n",
              "  'a',\n",
              "  'nice',\n",
              "  'speech',\n",
              "  '.'],\n",
              " 'sentence': ['The president Jesús Martínez arrived yesterday at Santa Cruz de Tenerife and he gave a nice speech.']}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOOK UP YOUR PREDICTING TEXT MODEL:\n",
        "-  `https://sparknlp.org/models`\n",
        "- Check limitations and task to accomplish (e.g., max number of tokens, embeddings, fill mask, sentiment, etc.)\n",
        "- Check size (some models can be very large)"
      ],
      "metadata": {
        "id": "Rk0uk9503fUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = PretrainedPipeline('analyze_sentimentdl_glove_imdb', 'en')"
      ],
      "metadata": {
        "id": "HZwOZyWD7oqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd80ad26-e001-45bd-cb4b-74579cd74844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "analyze_sentimentdl_glove_imdb download started this may take some time.\n",
            "Approx size to download 154.1 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test the pipeline with toy samples:"
      ],
      "metadata": {
        "id": "GQHDcUV41QF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = sentiment.annotate(\"The Minions is an excellent movie\")"
      ],
      "metadata": {
        "id": "w4hEnxkMDGiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnhVOwn23Gr1",
        "outputId": "b50bdfea-5d18-4ebc-d3c7-e7491e60b032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'document': ['The Minions is an excellent movie'],\n",
              " 'sentiment': ['pos'],\n",
              " 'word_embeddings': ['The', 'Minions', 'is', 'an', 'excellent', 'movie'],\n",
              " 'sentence_embeddings': ['The Minions is an excellent movie'],\n",
              " 'tokens': ['The', 'Minions', 'is', 'an', 'excellent', 'movie'],\n",
              " 'sentence': ['The Minions is an excellent movie']}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or use a spark pipeline to process a large dataset of texts:\n",
        "\n",
        "https://sparknlp.org/api/python/user_guide/annotators.html"
      ],
      "metadata": {
        "id": "lONKVLD71U__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *"
      ],
      "metadata": {
        "id": "8GnGRe-a3QPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        "  .setInputCol(\"text\") \\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained()\\\n",
        "  .setInputCols(\"document\") \\\n",
        "  .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "sentiment = SentimentDLModel.pretrained(\"sentimentdl_use_twitter\")\\\n",
        "  .setInputCols(\"sentence_embeddings\")\\\n",
        "  .setThreshold(0.7)\\\n",
        "  .setOutputCol(\"sentiment\")\n",
        "\n",
        "pipeline = Pipeline(stages=[documentAssembler, use, sentiment])\n",
        "\n",
        "data = spark.createDataFrame([[\"What a nasty movie.\"],[\"Indeed a good film.\"]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(data).transform(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5uhXaAm1a2t",
        "outputId": "8cfe3210-068d-4f85-9bcb-36f2545ba6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "sentimentdl_use_twitter download started this may take some time.\n",
            "Approximate size to download 11.4 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select(\"text\", \"sentiment.result\")\\\n",
        "      .selectExpr( \"text\", \"explode(result) as sentiment\")\\\n",
        "      .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEya08-BCj5v",
        "outputId": "b5c7b21a-e353-41ef-8f70-5a1901f1ebbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+\n",
            "|               text|sentiment|\n",
            "+-------------------+---------+\n",
            "|What a nasty movie.| negative|\n",
            "|Indeed a good film.| positive|\n",
            "+-------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing directly with embeddings:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-PRhIAYt4ZaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import Tokenizer, WordEmbeddingsModel, SentenceEmbeddings"
      ],
      "metadata": {
        "id": "sIB1YJH432op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        ".setInputCol(\"text\") \\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        ".setInputCols(\"document\") \\\n",
        ".setOutputCol(\"token\")\n",
        "\n",
        "embeddings = RoBertaEmbeddings.pretrained(\"roberta_embeddings_bertin_roberta_base_spanish\",\"es\") \\\n",
        ".setInputCols([\"document\", \"token\"]) \\\n",
        ".setOutputCol(\"embeddings\")\n",
        "\n",
        "pipeline = Pipeline(stages=[documentAssembler, tokenizer, embeddings])\n",
        "\n",
        "data = spark.createDataFrame([[\"Me encanta spark nlp\"],[\"No estoy seguro que sea bueno\"]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(data).transform(data)"
      ],
      "metadata": {
        "id": "ECstWRDu4deq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "-JPc6atcDNEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docClassifier = ClassifierDLApproach() \\\n",
        "    .setInputCols([\"sentence_embeddings\"]) \\\n",
        "    .setOutputCol(\"category\") \\\n",
        "    .setLabelColumn(\"label\") \\\n",
        "    .setBatchSize(64) \\\n",
        "    .setMaxEpochs(20) \\\n",
        "    .setLr(5e-3) \\\n",
        "    .setDropout(0.5)\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "    documentAssembler,\n",
        "    useEmbeddings,\n",
        "    docClassifier\n",
        "])\n",
        "pipelineModel = pipeline.fit(smallCorpus)\n",
        "\n",
        "#The result is a PipelineModel that can be used with transform(data) to classify sentiment."
      ],
      "metadata": {
        "id": "hHubY8tsCbh7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}