{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ekJDI7Osp83i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0426e17b-0238-417e-bb05-a039e65ca3a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=1288332cef15c09d2bfddec1e70b1eafac1c48ed4beb49b3c3a8a8f53bc25774\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "l9cNqN3ar32z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwkFogbVr8Rc",
        "outputId": "f0772224-eaf9-4201-eca2-f911d002284f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "# Importing the txt of the last notebook.\n",
        "# This txt is changed to reach better results, in the conclusions we will talk about the changes I made it.\n",
        "\n",
        "# Define the path to the txt file in your Google Drive\n",
        "google_drive_csv_path = \"/content/drive/MyDrive/UJI/Big Data/BigData NoteBooks/moviesmarks3\"\n",
        "\n",
        "# Define the path to save the txt file locally in your Colab environment\n",
        "local_csv_path = \"/content/moviesmark\"\n",
        "\n",
        "# Copy the CSV file from Google Drive to the local Colab environment\n",
        "shutil.copy(google_drive_csv_path, local_csv_path)\n",
        "\n",
        "# Now, you can access the file at local_csv_path and perform any operations you need"
      ],
      "metadata": {
        "id": "UYHqn0idk0WQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6efab4fd-49c9-47b9-8849-ea75c7912e96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/moviesmark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_model = spark.read.format('libsvm').load('/content/drive/MyDrive/UJI/Big Data/BigData NoteBooks/moviesmarks')"
      ],
      "metadata": {
        "id": "eLZCLlkljWUv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree classifier"
      ],
      "metadata": {
        "id": "2eo-rpasnHyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "metadata": {
        "id": "RR_UjNfkndkY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "feature_cols = [\"features\"]\n",
        "# Merges multiple columns into a vector column.\n",
        "assembler = VectorAssembler(inputCols = feature_cols, outputCol = \"features_vector\")\n",
        "\n",
        "# Split the data into training and test sets (20% held out for testing)\n",
        "(trainingData, testData) = df_model.randomSplit([0.8, 0.2], seed = 37)\n"
      ],
      "metadata": {
        "id": "FGWjmvmqihy9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "# Chain assembler and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, dt])\n",
        "\n",
        "# Train model.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)"
      ],
      "metadata": {
        "id": "MVY3omTqunEY"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Accuracy = %g \" % (accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn4EfnCsnDjn",
        "outputId": "c83acdbd-c13e-40ef-98e8-aa0ce5537d7f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       3.0|  1.0|(68514,[1,2,4,5,7...|\n",
            "|       3.0|  1.0|(68514,[2,3,4,5,6...|\n",
            "|       3.0|  1.0|(68514,[2,3,4,5,7...|\n",
            "|       3.0|  1.0|(68514,[2,3,4,5,7...|\n",
            "|       3.0|  1.0|(68514,[2,3,4,5,7...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Accuracy = 0.386189 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(maxIter=4, regParam=0.2)\n",
        "\n",
        "# Chain assembler and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, lr])\n",
        "\n",
        "# Train model.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Logistic Regression Accuracy = %g \" % (accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZUFzg3Nu17t",
        "outputId": "4c6eb547-2e38-4518-dfe7-3932e20e7615"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       2.0|  1.0|(68514,[1,2,4,5,7...|\n",
            "|       1.0|  1.0|(68514,[2,3,4,5,6...|\n",
            "|       2.0|  1.0|(68514,[2,3,4,5,7...|\n",
            "|       1.0|  1.0|(68514,[2,3,4,5,7...|\n",
            "|       2.0|  1.0|(68514,[2,3,4,5,7...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Logistic Regression Accuracy = 0.452685 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions\n",
        "Firstly, we will comment on the changes made to last week's Notebook document. We have made some changes in order to get a better result in the predictions. One of the first changes has been to filter out words that have a low IDF, namely greater than 7. This is because these words are words that are continuously repeated in the reviews and this could indicate that these words do not provide relevant information for our prediction, as they can be connectors, conjunctions or words that do not provide much meaning.\n",
        "This change has been made with the following code in the RDD IDF from previous practice:\n",
        "\n",
        "```\n",
        "IDF = IDF.filter(lambda x: x[1] > 7.0)\n",
        "```\n",
        "\n",
        "Another of the changes, focused in the same direction, has been to eliminate what we call stopwords. This has been done using the **nltk** package and the following instructions:\n",
        "\n",
        "\n",
        "```\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words('spanish'))\n",
        "```\n",
        "\n",
        "And then we filter in the **countWords** function of the previous Notebook.\n",
        "\n",
        "Finally, to carry out our prediction we have used two ML-based models: Decission Tree Algorithm and Logistic regression. On these we have modified some hyperparameters to obtain a better result. Thus obtaining that the Logistic Regression model has a higher accuracy. This may be due to several factors: Decision trees are non-linear models that can create complex decision boundaries. In the context of predicting film scores based on reviews, a decision tree may overfit the data by creating a complex tree structure that captures noise in the training data, resulting in lower accuracy on unseen data.\n",
        "Logistic Regression is a linear model that is generally simpler and less prone to overfitting. It models the relationship between input features and the target variable (film scores) in a more linear and interpretable manner.\n",
        "\n",
        "In addition, the quality and quantity of data also play a crucial role. In this case we may not have a large amount of data and the indicator we are using (tf*IDF) is too basic.\n",
        "\n",
        "But on balance, we have obtained very low accuracies, which may be due to the simplicity of these models and to the fact that they only create relationships between the occurrence of words and the attainment of certain scores. To obtain better results, it would be more appropriate to consider more features about the reviews and the words used.   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FsQtvHqW3q5r"
      }
    }
  ]
}